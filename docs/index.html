<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Laplacian Eigenmap Visualization</title>
    
    <!-- library for visualizaiton -->
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <!-- library for scrolling pages -->
    <script src="https://unpkg.com/scrollama"></script>
    <!-- library for displaying math formula -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="loading.css">

    <link rel="shortcut icon" type="image/x-icon" href="https://raw.githubusercontent.com/KevinBian107/laplacian_eigenmap_vis/master/images/logo.png" />
</head>


<body>
    <!-- Title Page -->
    <section id="title-page">
        <div class="title-container">
            <h1>An Intuitive Introduction to Laplacian Eigenmaps for Dimensionality Reduction</h1>
            <h3 class="authors"> By
                <a href="https://github.com/Wzhang3912">Weijie Zhang</a> and
                <a href="https://kevinbian107.github.io/">Kaiwen Bian</a>
            </h3>
            <h3 class="date">June, 2024</h3>
            <!-- <h3>Please use a bigger screen for your best visualization experience</h3> -->
            <!-- <button onclick="showTranslateOptions()" class="custom-translate-button">Translate</button> -->
            <p></p>
            <div id="google_translate_element"></div>
            <script type="text/javascript">
            // Initializes the Google Translate Element
            function googleTranslateElementInit() {
                new google.translate.TranslateElement({
                    pageLanguage: 'en',
                    includedLanguages: 'en,zh-CN,es,fr,de,ja,ru',
                    layout: google.translate.TranslateElement.InlineLayout.SIMPLE,
                    autoDisplay: true  // Change to true to allow automatic display
                }, 'google_translate_element');
            }

            // Function to manually trigger translation to Chinese
            function showTranslateOptions() {
                var selectField = document.querySelector(".goog-te-combo");
                if (selectField) {
                    selectField.value = 'zh-CN'; // Set the translation to Chinese
                    selectField.dispatchEvent(new Event('change')); // Trigger the change event
                } else {
                    console.error('Translation widget not loaded properly.');
                    // Optionally retry or handle the failure more gracefully
                }
            }

            // Optionally call showTranslateOptions on page load to set language immediately
            window.onload = function() {
                showTranslateOptions();
            };
            </script>
            <script type="text/javascript" src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script>
            </div>
        </div>
    </section>
    
    <section id='scroll'>
        <div class="scroll__container">

            <!--  step container  -->
            <div class='scroll__text'>
                <div class='step' data-step='1' id="step_1">
                    <h1>Introduction to Dimension Reudction</h1>
                    <p class="large-text">The images may take a moment to load...</p>
                    
                    <button id="loadButton">Load Visualization</button>

                    <p class="large-text">Here we have 300 artworks images from the collection at the Metropolitan Museum of Art (MET) to illustrate
                        the power of dimension reduction algorithm (i.e. Principal Component Analysis, Spectral Embeddings, ...). Each of the images 
                        in this dataset now lives in very high dimension (128 width x 128 height x 3 color channels) and for visualization purposes,
                        they are currently all positioned randomly on a 2D base.
                    </p>

                    <p class="large-text">Are there any "principals" in these 49,152 length vectors (flattening image) that can give us a better understanding for these images
                        maybe for a later task such as classification? This question have always been a important one to ask because high dimensional data are
                        very noisy and simple classfiers trained on high dimensional data aren't very robust and accurate in their predictions.
                    </p>
                    
                    <p class="large-text">Modern representation learning approaches lend us a way of looking at this problem, particularly for this visualization project, we are
                        adapting an spectral embedding dimension reduction technique knwon as Laplacien Eigenmap, which reduces the dimensionality of the dataset
                        while preserving the relationships between each data points. We will explain more in the following sections.
                    </p>

                </div>

                <div class='step hidden' data-step='2' id='step_2'>
                    <h1>From Data Points to Graph - KNN</h1>
                    <p class="large-text">Intuitively, what would be an valid way of preserving the relationship between each data point?
                        Perhaps we should know first what is the relationship between each data point. Again, these data point
                        lives in very high dimensional space, so mathamatically, we can find the "norm" or the Eucledian distance between these vectors
                        as a very naive but practical way of meausing similarities on such high dimension.
                    </p>

                    <p class="large-text">So we have a distance between each of the vectors with each other, how do we meausre who is close? How do we decide about what
                        is a good threshold of measurement of "close"? Again, we start with an intuitive approach called K Nearest Neighbor, which simply
                        deem the kth closest vectors to the current vectors as "close". Mathamatically, this is equivalent of making a matrix to represent
                        all of thepotential neighbors of a vector and then adding edges between each node (vector) i and its k closest neighbors.</p>
                        
                    <p class="large-text">We will extend such idea of "matrix" or similarity matrix in the later section. Now just play around with perfroming K nearest neighbors
                        on a subset of the image data set illustrated previously.
                    </p>
                    
                    <div id="sliderContainer">
                        <input type="range" min="1" max="14" value="2" class="slider" id="myRange">
                        <p>K =<span id="sliderValue">1</span></p>
                    </div>

                    <p class="large-text">(Hover over the image to view its neighbor. Click the image to "lock" the view and use the slider to expand to \(K\) Neighbors. Click the image again to "unlock" the view.)</p>
                </div>

                <div class='step hidden' data-step='3' id="step_3">
                    <h1>Measure of Closeness</h1>
                    <p class="large-text">We denote similarity matrix as W and degree matrix as D. In terms of KNN, 
                        similarity matrix encodes the information about the nodes and their respective neighbors. 
                        Degree matrix, on the other hand, is a diagnol matrix that encodes the number of neighrbos
                        for each node in its diagonal entries, which is calculated by the sum of each of the rows of
                        the similarity matrix. In the KNN case, degree matrix will be a diagonal matrix with diagonal entries k.<p>

                    <p class="large-text">A Similarity Matrix (6x6) here:</p>
                    <div class='matrix-wrapper'>
                        <div class='matrix' id="similarityMatrix"></div>
                        <div class='matrix-label'>= ùëä</div>
                    </div>

                    <p class="large-text">A Degree Matrix (6x6) here:</p>
                    <div class='matrix-wrapper'>
                        <div class='matrix' id="degreeMatrix"></div> 
                        <div class='matrix-label'>= ùê∑</div>
                    </div>

                    <p class="large-text">Then we can compute the Graph Laplacian Matrix ùêø = ùê∑ ‚àí ùëä <br>
                        &nbsp; ùê∑ is the degree matrix <br>
                        &nbsp; ùëä is the similarity matrix <br>
                    </p>

                    <p class="large-text">You may be wondering, what the heck are these numbers and matrix and how are they even
                        related to sloving the issue of extracting principals? Wait a little, we will expain in the following section.
                    </p>

                    <p class="large-text">(Hover over the images to view its neighbors and highlight in matrix.)</p>
                </div>

                <div class='step hidden' data-step='4' id="step_4">
                    <h1>Framing Optimization Problem</h1>
                    <p class="large-text">Don't be intemidated by the following scary looking math equation, it will be very easy to interpret
                        once you have read through this section. Now just forget everything we told you earlier with the linear algebra stuff (we
                        will use that later and you will also see the connections later). Now back to the problem of "preserving relationships on high dimension
                        to when the data is on low dimension", this is the equivalence of saying that "data points should be closer together when tehy are projected to
                        teh lower dimension". With this in mind, let's look at this equation:
                    </p>
                    
                    <p class="large-text"><span>$$ \text{Cost}(\hat{f}) = \sum_{i=1}^n \sum_{j=1}^n W_{ij} (f_i - f_j)^2 $$</span></p>
    
                    <p class="large-text">This equation is essentially saying that we are looking at a weighted cost function where data point \(f_i\) and \(f_j\) after projection would impose a larger cost
                        when they are far away from each other. Wait, but this doesn't seem right? Actually, there is the similarity matrix ùëä multiplied to this distance term to say that
                        "the bigger the term in ùëä, the more the distance weights in the overall cost" (in case of KNN, this value would be just 0 and 1 in ùëä). In this manner, when two data points are neighbors
                        of each other, represented by having a 1 in their corresponding \(i\) and \(j\) entries in the ùëä matrix and their distance after projecting to the lower dimension space would count towards
                        the cumulative cost of the projection.
                    </p>

                    <p class="large-text">Now this problem becomes a optimization porblem and, as data science majors, we love optimizatin problems because we can use different ways of looking
                        at the problem to find the optimal solution, in this case it would be the optimal "projected data point (each \(f_i, f_j\))" that minimize the cost function.
                    </p>

                </div>

                <div class='step hidden' data-step='5' id="step_5">
                    <h1>Laplacien Eigenmap In Eigenvector Space</h1>

                    <p class="large-text">Again, on the previous section, we introduced the way we frame the issue of dimension reduction
                        into a optimization issue. Now we will add a little bit more details into the picture. For the scope of this visualization,
                        we will not go into the mathamatical detail of the reasoning, but the optimization problem on the previous section can be reframed into a vector representation
                        in the following form:
                    </p>
                    <p class="large-text">$$ \text{Cost}(\hat{f}) = \hat{f}^T L \hat{f} $$</p>
                    <p class="large-text">subject to \( \|\hat{f}\| = 1 \) and \( \hat{f} \perp (1,1,\dots,1)^T \)</p>
                    
                    <p class="large-text">Doesn't this ùêø looks very familier? Yep, this is the exact Laplacien Matrix that we have introduced from the previous section, which is directly the key of
                        solving this optimization issue. Now we have framed the new optimization problem in a vector form, but we still need to find a way to minimize it. How should we do that? Again, for
                        mathamatical reasons that we will not go into in this visualization, the solution to this problem is exactly the bottom non-zero Eigenvalue Eigenvector of the Laplacien Matrix, which we have
                        graphed on the right with each images labeled on the data point (location of the image is decided by the bottom Eigenvectors). Observe the graph carefully and you may see some interesting trend (i.e. color intensity of the image, 
                        object ratio with respect to the image).
                    </p>

                    <p class="large-text">Click the dimension switch button to switch between looking at 1 Eigenvector projection and 2 Eigenvector projection adn observe differences in the "principal" found.</p>
                    <button id="transformButton"><span id="transformText">Reduce to 1 Dimensional Space</span></button>

                    <p class="large-text">Futher Reading: for those of you who want to go more into understanding the mathamatical aspects of Laplacien Eigenmap,
                        or about how the bottom Eigenvectors is the minimized solution (hint: relate to PCA), you can take DSC 140B from HDSI,
                        which we have linked lecture slides as below in the footer note section. The publication of the Laplacien Eigenmap algorithm, which was publisghed by one of our professor in HDSI.
                    </p>
                </div>

                <div class='step hidden' data-step='6' id="step_6">
                    <h1>Different Number For \(K\) Neighbors?</h1>

                    <p class="large-text"> Hyperparameter refers to the variables that we can set (not learned by the model). Remenber that there is a hyperparameter 
                        that you can tune for different performance fo the algorithm? That's right, the number of neighbors can significantly effect the similarity matrix created, 
                        which significantly chanegs teh result of the algorithm. 
                    Our default \(K\) previously was \(35\)</p>

                <!-- Dropdown menu for selecting K -->
                <label for="kDropdown" class="large-text">Select K:</label>
                <select id="kDropdown" class="large-text">
                    <option value="15">\(K\) = 15</option>
                    <option value="20">\(K\) = 20</option>
                    <option value="30">\(K\) = 30</option>
                    <option value="35">\(K\) = 35</option>
                    <option value="50">\(K\) = 50</option>
                    <option value="100">\(K\) = 100</option>
                </select>

                <p class="large-text">Select the number of \(K\) neighbors you want and click visualize to see the effect on the projection! 
                    Maybe try to identify some differences in the principals captured by different \(K\) values. This also illustrate another usage of 
                    this algorithm: capturing some image with specific characteristics for filtering purposes (i.e. the art pieces image).
                </p>

                <!-- Button to transform images based on selected K -->
                <button id="kEffectButton">Visualize Different \(K\)</button>

                <p class="large-text">Preserving Locality: Notice that it seems like the underlaying principal extracted (trend seen) is somewhat the same, 
                    illustrating underlaying mechanism of Laplacien Eigenmap! Try to switch between \(K=20\) and \(K=30\).</p>
            
                <!-- Dropdown menu for selecting K -->
                <label for="kDropdown" class="large-text">Transform From </label>
                <select id="kDropdown_from" class="large-text">
                    <option value="15">\(K\) = 15</option>
                    <option value="20">\(K\) = 20</option>
                    <option value="30">\(K\) = 30</option>
                    <option value="35">\(K\) = 35</option>
                    <option value="50">\(K\) = 50</option>
                    <option value="100">\(K\) = 100</option>
                </select>

                <!-- Dropdown menu for selecting K -->
                <label for="kDropdown" class="large-text"> To </label>
                <select id="kDropdown_to" class="large-text">
                    <option value="15">\(K\) = 15</option>
                    <option value="20">\(K\) = 20</option>
                    <option value="30">\(K\) = 30</option>
                    <option value="35">\(K\) = 35</option>
                    <option value="50">\(K\) = 50</option>
                    <option value="100">\(K\) = 100</option>
                </select>

                <!-- Button to transform images based on selected K -->
                <button id="kFromToButton">Visualize Transformation</button>

                <p class="large-text hidden warning" id="warning">The two \(K\) must not be the same.</p>
                
                </div>

            </div>
            <div class="scroll__vis">
                <!-- svg will be created here using d3 -->
                <div id="loader-container" class="hidden">
                    <div class="lds-roller"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div>
                </div>
                <div id="imageVis"></div>
                <div id="annoationVis"></div>
                <div id="knnVis"></div>
                <div id="matrixKnnVis"></div>
                <div id="linkVis"></div>
            </div>
        </div>

    </section>
    
    <footer id='footer' class="footer hidden">
        <div class="footer-content">
          <h3>References</h3>
          <ul>
            <li>Inspired by: <a href="https://dimensionality-reduction-293e465c2a3443e8941b016d.vercel.app/" target="_blank">The Beginner's Guide to Dimensionality Reduction</a>
            <li>DSC 140B: <a href="https://dsc-courses.github.io/dsc140b-2023-sp/materials/lectures/schedules/tr/09-laplacian_eigenmaps/slides.pdf" target="_blank">Laplacien Eigenmap</a></li>
            <li>DSC 140B: <a href="https://dsc-courses.github.io/dsc140b-2023-sp/materials/lectures/schedules/tr/06-pca-I/slides.pdf" target="_blank">PCA</a></li>
            <li>Original Paper: <a href="https://www2.imm.dtu.dk/projects/manifold/Papers/Laplacian.pdf" target="_blank">Laplacian Eigenmaps for Dimensionality Reduction and Data Representation</a></li>
          </ul>
        </div>
         <!-- GitHub Icon -->
        <a href="https://github.com/KevinBian107/laplacian_eigenmap_vis" target="_blank" aria-label="GitHub" class="footer-icon">
            <svg height="32" viewBox="0 0 16 16" version="1.1" width="32" aria-hidden="true">
                <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38C13.71 14.53 16 11.54 16 8c0-4.42-3.58-8-8-8z"></path>
            </svg>
        </a>
        <div class="footer-bottom">
          <p>Copyright ¬© 2024 Weijie Zhang & Kaiwewn Bian. All rights reserved.</p>
        </div>
      </footer>

    <!-- <script src="main.js"></script> -->
    <script type="module" src="scroll.js"></script>
    <script scr="main.js"></script>
    <script scr='knn.js'></script>
    <script src="translate.js"></script>
      
</body>
</html>

