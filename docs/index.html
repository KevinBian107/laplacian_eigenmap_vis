<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Laplacian Eigenmap Visualization</title>
    
    <!-- libraries for visualizaiton -->
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="https://unpkg.com/scrollama"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <link rel="stylesheet" href="style.css">
</head>

<body>
    <!-- Title Page -->
    <section id="title-page">
        <div class="title-container">
            <h1>An Intuitive Introduction to Laplacian Eigenmaps for Dimensionality Reduction</h1>
            <h3 class="authors">By Weijie Zhang and Kaiwen Bian</h3>
            <h3 class="date">2024</h3>
            <h3>Please use a bigger screen for your best visualization experience</h3>
            <p>Prototype: in development</p>
        </div>
    </section>
    
    <section id='scroll'>
        <div class="scroll__container">

            <!--  step container  -->
            <div class='scroll__text'>
                <div class='step' data-step='1'>
                    <h1>Introduction to Dimension Reudction</h1>
                    <p class="large-text">Here we have 300 artworks images from the collection at the Metropolitan Museum of Art (MET) to illustrate
                        the power of dimension reduction algorithm (i.e. Principal Component Analysis, Spectral Embeddings, ...). Each of the images 
                        in this dataset now lives in very high dimension (128 width x 128 height x 3 color channels) and for visualization purposes,
                        they are currently all positioned randomly on a 2D base.
                    </p>

                    <p class="large-text">Are there any "principals" in these 49,152 length vectors (flattening image) that can give us a better understanding for these images
                        maybe for a later task such as classification? This question have always been a important one to ask because high dimensional data are
                        very noisy and simple classfiers trained on high dimensional data aren't very robust and accurate in their predictions.
                    </p>
                    
                    <p class="large-text">Modern representation learning approaches lend us a way of looking at this problem, particularly for this visualization project, we are
                        adapting an spectral embedding dimension reduction technique knwon as Laplacien Eigenmap, which reduces the dimensionality of the dataset
                        while preserving the relationships between each data points. We will explain more in the following sections.
                    </p>

                </div>

                <div class='step' data-step='2'>
                    <h1>From Data Points to Graph - KNN</h1>
                    <p class="large-text">Intuitively, what would be an valid way of preserving the relationship between each data point?
                        Perhaps we should know first what is the relationship between each data point. Again, these data point
                        lives in very high dimensional space, so mathamatically, we can find the "norm" or the Eucledian distance between these vectors
                        as a very naive but practical way of meausing similarities on such high dimension.
                    </p>

                    <p class="large-text">So we have a distance between each of the vectors with each other, how do we meausre who is close? How do we decide about what
                        is a good threshold of measurement of "close"? Again, we start with an intuitive approach called K Nearest Neighbor, which simply
                        deem the kth closest vectors to the current vectors as "close". Mathamatically, this is equivalent of making a matrix to represent
                        all of thepotential neighbors of a vector and then adding edges between each node (vector) i and its k closest neighbors.</p>
                        
                    <p class="large-text">We will extend such idea of "matrix" or similarity matrix in the later section. Now just play around with perfroming K nearest neighbors
                        on a subset of the image data set illustrated previously.
                    </p>
                    
                    <div id="sliderContainer">
                        <input type="range" min="1" max="14" value="2" class="slider" id="myRange">
                        <p>K =<span id="sliderValue">1</span></p>
                    </div>
                    <p>...</p>

                </div>

                <div class='step' data-step='3'>
                    <h1>Measure of Closeness</h1>
                    <p class="large-text">We denote similarity matrix as W and degree matrix as D. In terms of KNN, 
                        similarity matrix encodes the information about the nodes and their respective neighbors. 
                        Degree matrix, on the other hand, is a diagnol matrix that encodes the number of neighrbos
                        for each node in its diagonal entries, which is calculated by the sum of each of the rows of
                        the similarity matrix. In the KNN case, degree matrix will be a diagonal matrix with diagonal entries k.<p>

                    <p class="large-text">Try to hover over the images to view its neighbors</p>

                    <p class="large-text">A Similarity Matrix (6x6) here:</p>
                    <div class='matrix-wrapper'>
                        <div class='matrix' id="similarityMatrix"></div>
                        <div class='matrix-label'>= ùëä</div>
                    </div>

                    <p class="large-text">A Degree Matrix (6x6) here:</p>
                    <div class='matrix-wrapper'>
                        <div class='matrix' id="degreeMatrix"></div> 
                        <div class='matrix-label'>= ùê∑</div>
                    </div>

                    <p class="large-text">Then we can compute the Graph Laplacian Matrix ùêø = ùê∑ ‚àí ùëä <br>
                        &nbsp; ùê∑ is the degree matrix <br>
                        &nbsp; ùëä is the similarity matrix <br>
                    </p>

                    <p class="large-text">You may be wondering, what the heck are these numbers and matrix and how are they even
                        related to sloving the issue of extracting principals? Wait a little, we will expain in the following section.
                    </p>
                </div>

                <div class='step' data-step='5'>
                    <h1>Framing Optimization Problem</h1>
                    <p class="large-text">Don't be intemidated by the following scary looking math equation, it will be very easy to interpret
                        once you have read through this section. Now just forget everything we told you earlier with the linear algebra stuff (we
                        will use that later and you will also see the connections later). Now back to the problem of "preserving relationships on high dimension
                        to when the data is on low dimension", this is the equivalence of saying that "data points should be closer together when tehy are projected to
                        teh lower dimension". With this in mind, let's look at this equation:
                    </p>
                    
                    <p class="large-text"><span>$$ \text{Cost}(\hat{f}) = \sum_{i=1}^n \sum_{j=1}^n W_{ij} (f_i - f_j)^2 $$</span></p>
    
                    <p class="large-text">This equation is essentially saying that we are looking at a weighted cost function where data point \(f_i\) and \(f_j\) after projection would impose a larger cost
                        when they are far away from each other. Wait, but this doesn't seem right? Actually, there is the similarity matrix ùëä multiplied to this distance term to say that
                        "the bigger the term in ùëä, the more the distance weights in the overall cost" (in case of KNN, this value would be just 0 and 1 in ùëä). In this manner, when two data points are neighbors
                        of each other, represented by having a 1 in their corresponding \(i\) and \(j\) entries in the ùëä matrix and their distance after projecting to the lower dimension space would count towards
                        the cumulative cost of the projection.
                    </p>

                    <p class="large-text">Now this problem becomes a optimization porblem and, as data science majors, we love optimizatin problems because we can use different ways of looking
                        at the problem to find the optimal solution, in this case it would be the optimal "projected data point (each \(f_i, f_j\))" that minimize the cost function.
                    </p>

                </div>

                <div class='step' data-step='6'>
                    <h1>Laplacien Eigenmap In Eigenvector Space</h1>
                    <p class="large-text">Again, on the previous section, we introduced the way we frame the issue of dimension reduction
                        into a optimization issue. Now we will add a little bit more details into the picture. For the scope of this visualization,
                        we will not go into the mathamatical detail of the reasoning, but the optimization problem on the previous section can be reframed into a vector representation
                        in the following form:
                    </p>
                    <p class="large-text">$$ \text{Cost}(\hat{f}) = \hat{f}^T L \hat{f} $$</p>
                    <p class="large-text">subject to \( \|\hat{f}\| = 1 \) and \( \hat{f} \perp (1,1,\dots,1)^T \)</p>
                    
                    <p class="large-text">Doesn't this ùêø looks very familier? Yep, this is the exact Laplacien Matrix that we have introduced from the previous section, which is directly the key of
                        solving this optimization issue. Now we have framed the new optimization problem in a vector form, but we still need to find a way to minimize it. How should we do that? Again, for
                        mathamatical reasons that we will not go into in this visualization, the solution to this problem is exactly the bottom non-zero Eigenvalue Eigenvector of the Laplacien Matrix, which we have
                        graphed on the right with each images labeled on the data point (location of the image is decided by the bottom Eigenvectors). Observe the graph carefully and you may see some interesting trend (i.e. color intensity of the image). 
                        Hit the dimension switch button to switch between looking at 1 Eigenvector projection and 2 Eigenvector projection.
                    </p>

                    <p class="large-text">For those of you who want to go more into understanding the mathamatical aspects of Laplacien Eigenmap, about the constrain given above, the matrix optimization problem formulation, 
                        or about how the bottom Eigenvectors is the minimized solution (hint: relate to PCA), you can take DSC 140Bm from HDSI, which we have linked lecture slides as below.
                    </p>

                    <p class="large-text">If you feel like these are too easy, here is the publication of the Laplacien Eigenmap algorithm, which was publisghed by one of our professor in HDSI.</p>

                    <button onclick="window.location.href='https://dsc-courses.github.io/dsc140b-2023-sp/materials/lectures/schedules/tr/09-laplacian_eigenmaps/slides.pdf'">
                        140B Laplacien Eigenmap
                    </button>

                    <button onclick="window.location.href='https://dsc-courses.github.io/dsc140b-2023-sp/materials/lectures/schedules/tr/06-pca-I/slides.pdf'">
                        140B PCA
                    </button>

                    <button onclick="window.location.href='https://www2.imm.dtu.dk/projects/manifold/Papers/Laplacian.pdf'">
                        Laplacian Eigenmap Algorithm
                    </button>
                </div>

            </div>
            <div class="scroll__vis">
                <!-- svg will be created here using d3 -->
                <div id="imageVis"></div>
                <div id="matrixKnnVis"></div>
                <div id="linkVis"></div>
            </div>
        </div>

    </section>    

    <!-- <script src="main.js"></script> -->
    <script type="module" src="scroll.js"></script>
    <script scr="main.js"></script>
    <script scr='knn.js'></script>
</body>
</html>

